\documentclass[11pt]{book}
\usepackage{amsmath, amssymb, amsthm, bm, graphicx, hyperref, algorithm, algorithmic, mathtools, physics}
\usepackage{tikz}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]

\title{Mathematical Foundations for Advanced Generative Models}
\author{Prepared for Graduate Students in Machine Learning}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
Generative modeling has become one of the most influential areas in modern machine learning, powering applications in image synthesis, text generation, and scientific modeling. To fully understand architectures like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, and Energy-Based Models, one requires a deep foundation in mathematics: linear algebra, probability and statistics, optimization, and stochastic processes.

These notes are designed as a crash course for graduate students, blending mathematical rigor with application-driven intuition. Each concept is introduced with its historical origin, motivation for machine learning, and its formal theoretical underpinning. We assume familiarity with calculus and basic probability; all other prerequisites are developed within the text.

\section{Structure of the Notes}
These notes progress from foundational mathematics to modern generative modeling. Chapters~2--5 develop linear algebra, probability, optimization, and stochastic processes. Chapter~6 synthesizes these tools to study VAEs, GANs, diffusion models, energy-based models, autoregressive models, and normalizing flows.

\section{Required Background}
The reader should be comfortable with multivariable calculus, basic probability, and programming in a language such as Python. Knowledge of measure theory and advanced analysis is helpful but not required; essential concepts are introduced as needed.

\section{Notation}
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \hline
        Symbol & Meaning \\ \hline
        $\mathbb{R}^n$ & $n$-dimensional real vector space \\
        $\mathbf{x}$ & column vector \emph{x} \\
        $\|x\|_2$ & Euclidean norm of $x$ \\
        $\nabla f$ & gradient of function $f$ \\
        $\mathbb{E}[X]$ & expectation of random variable $X$ \\
        $\mathrm{Var}(X)$ & variance of $X$ \\
        $I_n$ & $n\times n$ identity matrix \\
        $\mathcal{N}(\mu,\Sigma)$ & Gaussian with mean $\mu$ and covariance $\Sigma$ \\
        \hline
    \end{tabular}
    \caption{Common notation used throughout the notes.}
\end{table}

\section{References and Further Reading}
For a deeper exploration of the mathematics presented here, see texts such as \emph{Linear Algebra Done Right} by Axler, \emph{Probability and Measure} by Billingsley, and \emph{Convex Optimization} by Boyd and Vandenberghe. Historical notes and citations are provided in the bibliography.

\chapter{Linear Algebra Foundations}
Linear algebra provides the language in which data and model parameters are represented. Understanding vector spaces, eigenvalues, and matrix factorization is essential for designing and analyzing generative models.

\section{Vector Spaces and Norms}
\subsection{Introduction}
\begin{definition}[Vector Space]
A \emph{vector space} $V$ over a field $\mathbb{F}$ is a set equipped with two operations, vector addition and scalar multiplication, satisfying the eight axioms of associativity, commutativity, existence of identity and inverses, and distributivity. Elements of $V$ are called \emph{vectors}.
\end{definition}
Vector spaces form the backbone of data representation in machine learning. Typical examples include $\mathbb{R}^n$, spaces of matrices, and function spaces such as $L^2([0,1])$.

\subsection{Why Needed}
Neural networks, embeddings, and covariance matrices all rely on linear algebra. For GANs and VAEs, data lives in high-dimensional vector spaces, and understanding subspaces and projections is essential for latent representations. Many optimization algorithms exploit linear structure to compute gradients and updates efficiently.

\subsection{Details and Concepts}
Let $V$ be a finite-dimensional vector space over $\mathbb{R}$ with basis $\{v_1,\dots,v_n\}$. Every $x\in V$ can be uniquely expressed as $x=\sum_{i=1}^n x_i v_i$. The \emph{dimension} of $V$ is $n$.

\begin{definition}[Norm]
A \emph{norm} on $V$ is a function $\|\cdot\|:V\to[0,\infty)$ such that for all $x,y\in V$ and $\alpha\in\mathbb{R}$:
\begin{enumerate}
    \item $\|x\|=0$ iff $x=0$,
    \item $\|\alpha x\|=|\alpha|\,\|x\|$,
    \item $\|x+y\|\le \|x\|+\|y\|$ (triangle inequality).
\end{enumerate}
\end{definition}
Important norms on $\mathbb{R}^n$ include the $\ell_p$ norms
\begin{equation}
\|x\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{1/p},\qquad 1\le p<\infty,
\end{equation}
and the $\ell_\infty$ norm $\|x\|_\infty=\max_i|x_i|$. Norms induce metrics and topologies, providing notions of distance and convergence essential for analyzing algorithms.

\subsection{Theorems and Contributions}
\begin{theorem}[Cauchy--Schwarz Inequality \cite{cauchy1821,schwarz1888}]
For any $x,y\in\mathbb{R}^n$,
\begin{equation}
|\langle x,y\rangle|\le \|x\|_2\,\|y\|_2.
\end{equation}
\end{theorem}
\begin{proof}
Consider the nonnegative quadratic $q(t)=\|x-ty\|_2^2=\langle x-ty,x-ty\rangle$. Expanding yields $q(t)=\|x\|_2^2-2t\langle x,y\rangle+t^2\|y\|_2^2$. The discriminant must be nonpositive, giving $4\langle x,y\rangle^2-4\|x\|_2^2\|y\|_2^2\le0$.
\end{proof}
This inequality underlies correlation measures and is crucial for bounding errors in machine learning algorithms. Equality holds if and only if $x$ and $y$ are linearly dependent.

\section{Eigenvalues and Eigenvectors}
\subsection{Introduction}
Let $A\in\mathbb{R}^{n\times n}$. A nonzero vector $v\in\mathbb{R}^n$ is an \emph{eigenvector} of $A$ with associated \emph{eigenvalue} $\lambda$ if $Av=\lambda v$. Eigenvalues describe how linear transformations scale vectors, and eigenvectors describe invariant directions.

\subsection{Why Needed}
In VAEs, covariance matrices of Gaussian priors use eigen-decompositions. In GAN stability, eigenvalue analysis explains training oscillations. In diffusion modeling, spectral analysis governs smoothing operations and the behavior of Laplacians on graphs or continuous domains.

\subsection{Mathematical Details}
The eigenvalues of $A$ are roots of its characteristic polynomial $p_A(\lambda)=\det(A-\lambda I)$. For diagonalizable matrices we have the factorization $A=V\Lambda V^{-1}$, where $\Lambda$ is diagonal. If $A$ is symmetric, the following fundamental result applies.

\begin{theorem}[Spectral Theorem \cite{hilbert1904}]
If $A\in\mathbb{R}^{n\times n}$ is symmetric, then there exists an orthogonal matrix $Q$ and a diagonal matrix $\Lambda$ such that $A=Q\Lambda Q^T$. The diagonal entries of $\Lambda$ are the real eigenvalues of $A$, and the columns of $Q$ form an orthonormal basis of eigenvectors.
\end{theorem}
\begin{proof}[Proof Sketch]
By induction on $n$ and the fact that symmetric matrices have real eigenvalues, one constructs an orthonormal eigenbasis using Gram--Schmidt orthogonalization.
\end{proof}
Principal Component Analysis (Pearson, 1901) applies the spectral theorem to the covariance matrix $\Sigma=\tfrac1m\sum_{i=1}^m x_ix_i^T$, projecting data onto leading eigenvectors to reduce dimensionality.

\section{Matrix Operations and Calculus}
\subsection{Matrix Operations}
Matrix addition and multiplication follow the usual rules $(A+B)_{ij}=A_{ij}+B_{ij}$ and $(AB)_{ij}=\sum_k A_{ik}B_{kj}$. The trace $\operatorname{tr}(A)=\sum_i A_{ii}$ is linear and satisfies $\operatorname{tr}(AB)=\operatorname{tr}(BA)$. The Frobenius norm $\|A\|_F=\sqrt{\sum_{ij}A_{ij}^2}$ extends the Euclidean norm and is useful for measuring reconstruction error in VAEs.

\subsection{Singular Value Decomposition}
\begin{theorem}[Singular Value Decomposition]
Any $A\in\mathbb{R}^{m\times n}$ admits a factorization $A=U\Sigma V^T$ where $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ are orthogonal and $\Sigma$ is diagonal with nonnegative entries $\sigma_1\ge\dots\ge\sigma_r>0$. The values $\sigma_i$ are the \emph{singular values} of $A$.
\end{theorem}
SVD underlies low-rank approximations and is central to matrix completion and dimensionality reduction.

\subsection{Matrix Calculus for Backpropagation}
Matrix derivatives obey rules analogous to scalar calculus. For $f(X)=\operatorname{tr}(AX^T)$ we have $\nabla_X f=A$. If $f(X)=\|AX-b\|_2^2$, then $\nabla_X f=2A^T(AX-b)$. These formulas enable backpropagation through linear layers and are generalized via the chain rule to deep networks.

\subsection{Tensor Notation}
Higher-order data such as images and video require tensors. A tensor $\mathcal{T}\in\mathbb{R}^{n_1\times\cdots\times n_k}$ has entries $\mathcal{T}_{i_1\dots i_k}$. Operations like contraction generalize matrix multiplication and appear in attention mechanisms and diffusion model score networks.

\chapter{Probability and Statistics Foundations}
Probabilistic modeling underpins generative approaches by describing uncertainty and variability in data and latent variables.

\section{Random Variables and Distributions}
\subsection{Introduction}
A \emph{random variable} $X$ on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ is a measurable function $X:\Omega\to\mathbb{R}$ assigning numerical values to outcomes of an experiment.

\subsection{Why Needed}
VAEs assume latent variables $z$ with Gaussian priors; GANs generate samples from latent distributions; diffusion models explicitly define forward stochastic processes and require understanding of conditional distributions and Markov chains.

\subsection{Details}
A discrete random variable has a probability mass function (PMF) $P(X=x)$ with $\sum_x P(X=x)=1$. A continuous random variable has a probability density function (PDF) $p(x)$ such that $\int_{\mathbb{R}} p(x)\,dx=1$.

The expectation of a function $g$ under $p$ is $\mathbb{E}[g(X)]=\int g(x)p(x)\,dx$. The variance is $\operatorname{Var}(X)=\mathbb{E}[(X-\mu)^2]$, where $\mu=\mathbb{E}[X]$.

Two random variables $X$ and $Y$ are \emph{independent} if $p_{XY}(x,y)=p_X(x)p_Y(y)$ for all $(x,y)$. Independence often simplifies modeling and allows factorization of joint distributions.

\subsection{Theorems and Contributions}
\begin{theorem}[Law of Large Numbers \cite{bernoulli1713}]
Let $\{X_i\}$ be i.i.d. with mean $\mu$. Then the sample mean $\bar{X}_n=\frac1n\sum_{i=1}^n X_i$ satisfies $\bar{X}_n\to\mu$ almost surely as $n\to\infty$.
\end{theorem}
\begin{proof}[Proof Sketch]
Apply Kolmogorov's strong law for i.i.d. variables with finite variance.
\end{proof}

\begin{theorem}[Central Limit Theorem \cite{laplace1812,lyapunov1901}]
Let $\{X_i\}$ be i.i.d. with mean $\mu$ and variance $\sigma^2<\infty$. Then
\begin{equation}
\frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \xrightarrow{d} \mathcal{N}(0,1).
\end{equation}
\end{theorem}
This explains the prevalence of Gaussian assumptions in generative modeling.

\section{Information Theory}
\subsection{Introduction}
Information theory quantifies uncertainty and divergence between distributions. It was initiated by Shannon in 1948 and underlies loss functions used in generative modeling.

\subsection{Why Needed}
VAEs employ Kullback--Leibler (KL) divergence in the Evidence Lower Bound (ELBO). GANs can be interpreted via Jensen--Shannon divergence. Diffusion models rely on score functions linked to Fisher information.

\subsection{Details}
\begin{definition}[Entropy]
For a discrete random variable $X$ with PMF $p(x)$, the Shannon entropy is
\begin{equation}
H(X) = -\sum_x p(x)\log p(x).
\end{equation}
\end{definition}

For continuous $X$, the differential entropy is $h(X)=-\int p(x)\log p(x)\,dx$. The cross-entropy between $p$ and $q$ is $H(p,q)=-\int p(x)\log q(x)\,dx$.

\begin{definition}[Kullback--Leibler Divergence \cite{kullback1951}]
The KL divergence from $p$ to $q$ is
\begin{equation}
D_{\mathrm{KL}}(p\|q) = \int p(x) \log \frac{p(x)}{q(x)}\, dx.
\end{equation}
\end{definition}
KL divergence is nonnegative and equals zero if and only if $p=q$ almost everywhere.

Mutual information between random variables $X$ and $Y$ is
\begin{equation}
I(X;Y)=D_{\mathrm{KL}}(p_{XY}\|p_X p_Y)=H(X)+H(Y)-H(X,Y),
\end{equation}
measuring the reduction in uncertainty of one variable given the other.

\subsection{Theorems and Contributions}
\begin{theorem}[Gibbs' Inequality \cite{gibbs1873,shannon1948}]
For distributions $p$ and $q$, $D_{\mathrm{KL}}(p\|q)\ge0$ with equality iff $p=q$ a.e.
\end{theorem}
\begin{proof}
Apply Jensen's inequality to the convex function $\phi(t)=t\log t$.
\end{proof}

\begin{theorem}[Jensen's Inequality \cite{jensen1906}]
For a convex function $f$ and random variable $X$, $f(\mathbb{E}[X])\le\mathbb{E}[f(X)]$.
\end{theorem}
This inequality is the basis for deriving the ELBO in variational inference.

\section{Bayes' Theorem and Conditional Expectation}
Bayes (1763) showed how to invert conditional probabilities:
\begin{equation}
\mathbb{P}(A\mid B)=\frac{\mathbb{P}(B\mid A)\mathbb{P}(A)}{\mathbb{P}(B)}.
\end{equation}
In terms of densities, $p(x\mid y)=p(y\mid x)p(x)/p(y)$. The conditional expectation of $X$ given a $\sigma$-algebra $\mathcal{G}$ is the unique $\mathcal{G}$-measurable random variable $\mathbb{E}[X\mid\mathcal{G}]$ satisfying $\int_G \mathbb{E}[X\mid\mathcal{G}]\, d\mathbb{P}=\int_G X\, d\mathbb{P}$ for all $G\in\mathcal{G}$. Conditional expectations generalize least-squares projections and appear in VAEs when computing expectations over latent variables.

\section{Covariance and Independence}
For random variables $X$ and $Y$, the covariance is
\begin{equation}
\operatorname{Cov}(X,Y)=\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])].
\end{equation}
If $X$ and $Y$ are independent then $\operatorname{Cov}(X,Y)=0$, but the converse need not hold. Uncorrelated Gaussian variables are independent, a property exploited in the diagonal Gaussian priors of VAEs.

\section{Parameter Estimation: MLE and MAP}
Given data $x_1,\dots,x_n$ drawn i.i.d. from $p(x\mid\theta)$, the maximum likelihood estimator (MLE) solves
\begin{equation}
\hat{\theta}_{\mathrm{MLE}}=\arg\max_{\theta} \prod_{i=1}^n p(x_i\mid\theta).
\end{equation}
With a prior $p(\theta)$, the maximum a posteriori (MAP) estimator is
\begin{equation}
\hat{\theta}_{\mathrm{MAP}}=\arg\max_{\theta} p(\theta\mid x_{1:n})=\arg\max_{\theta} p(\theta)\prod_{i=1}^n p(x_i\mid\theta).
\end{equation}
MLE and MAP provide point estimates used to initialize parameters in generative models.

\section{Bayesian Inference}
Bayesian inference updates beliefs through the posterior $p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{p(x)}$. Predictions marginalize over parameters: $p(x_{\text{new}}\mid x)=\int p(x_{\text{new}}\mid\theta)p(\theta\mid x)\, d\theta$. Variational inference approximates the posterior by a tractable family, leading to the ELBO optimized by VAEs.

\section{Concentration Inequalities}
Concentration bounds quantify deviations of random variables from their expectations. For independent bounded $X_i\in[a,b]$, Hoeffding (1963) proved
\begin{equation}
\mathbb{P}\left(\frac1n\sum_{i=1}^n X_i-\mathbb{E}[X_i]\ge t\right)\le\exp\left(-\frac{2n t^2}{(b-a)^2}\right).
\end{equation}
Such results justify generalization guarantees and the design of learning rates in stochastic optimization.

\section{Monte Carlo Methods and MCMC}
Monte Carlo integration approximates expectations via samples: $\mathbb{E}[f(X)]\approx\frac1N\sum_{i=1}^N f(x_i)$. When direct sampling is hard, Markov Chain Monte Carlo (MCMC) constructs a Markov chain with stationary distribution $p$. The Metropolis--Hastings algorithm proposes $x'\sim q(x'\mid x)$ and accepts with probability $\alpha=\min\{1, \frac{p(x')q(x\mid x')}{p(x)q(x'\mid x)}\}$. MCMC underlies training of EBMs where sampling from $p_\theta$ is required.

\chapter{Optimization Foundations}
Optimization provides algorithms for training generative models by minimizing or maximizing objective functions. We distinguish between deterministic algorithms, which use exact gradients, and stochastic algorithms that rely on noisy gradient estimates from data samples.

\section{Motivation and Overview}
Modern machine learning models often contain millions of parameters and are trained on massive datasets. Computing exact gradients of the empirical risk can be prohibitively expensive, motivating iterative methods that approximate these gradients with stochastic samples. Optimization theory provides guarantees about when such procedures converge and how quickly.

\section{Optimization Preliminaries}
\subsection{Convexity}
\begin{definition}[Convex Function]
A function $f: \mathbb{R}^n\to\mathbb{R}$ is \emph{convex} if for all $x,y$ and $\lambda\in[0,1]$,
\begin{equation}
f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y).
\end{equation}
\end{definition}
Convex problems enjoy the property that any local minimum is global. Many losses used in machine learning, such as logistic regression, are convex in their parameters.

\subsection{Lipschitz Continuity and Smoothness}
\begin{definition}[Lipschitz Gradient]
We say $f$ has an $L$-Lipschitz gradient if
\begin{equation}
\|\nabla f(x)-\nabla f(y)\|_2\le L\|x-y\|_2 \qquad \forall x,y.
\end{equation}
\end{definition}
When $f$ is twice differentiable this is equivalent to $\|\nabla^2 f(x)\|_2\le L$, giving a bound on curvature. Smoothness enables quadratic upper bounds that are central to convergence analysis.

\subsection{Strong Convexity}
\begin{definition}[Strong Convexity]
A differentiable function $f$ is \emph{$\mu$-strongly convex} if
\begin{equation}
f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}\|y-x\|_2^2 \qquad \forall x,y.
\end{equation}
\end{definition}
Strong convexity implies the minimizer $x^*$ is unique and guarantees linear convergence of many algorithms.

\subsection{Basic Convergence Result}
\begin{theorem}
If $f$ is convex and differentiable with an $L$-Lipschitz gradient, any point with $\nabla f(x)=0$ is a global minimizer of $f$.
\end{theorem}
\begin{proof}
By convexity, $f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle$ for all $y$. Setting $\nabla f(x)=0$ yields $f(y)\ge f(x)$.
\end{proof}

\section{Gradient Descent}
\subsection{Introduction}
Gradient descent iteratively updates parameters in the direction of steepest descent to minimize a differentiable function.

\subsection{Why Needed}
VAEs optimize the ELBO; GANs solve min--max games using gradient-based updates; diffusion models optimize denoising objectives. Understanding convergence guarantees informs learning rate schedules and regularization.

\subsection{Details}
Given a differentiable function $f: \mathbb{R}^n \to \mathbb{R}$, gradient descent with step size $\eta>0$ updates
\begin{equation}
x_{k+1} = x_k - \eta \nabla f(x_k).
\end{equation}

\begin{algorithm}
\caption{Gradient Descent}
\begin{algorithmic}[1]
\STATE Initialize $x_0$
\FOR{$k=0,1,2,\dots$}
    \STATE $x_{k+1}=x_k-\eta\nabla f(x_k)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Theorems and Contributions}
\begin{theorem}[Convergence for $L$-Smooth Convex Functions \cite{cauchy1847,rockafellar1970}]
Suppose $f$ is convex and has $L$-Lipschitz gradient. If $0<\eta<2/L$, then gradient descent satisfies
\begin{equation}
f(x_k)-f(x^*)\le\frac{\|x_0-x^*\|_2^2}{2\eta k},
\end{equation}
where $x^*$ is a minimizer of $f$.
\end{theorem}
\begin{proof}[Proof Sketch]
Convexity implies $f(x_{k+1})\le f(x_k)+\langle\nabla f(x_k),x_{k+1}-x_k\rangle+\tfrac{L}{2}\|x_{k+1}-x_k\|^2$. Substituting the update and telescoping yields the bound.
\end{proof}

\section{Newton's Method}
\subsection{Introduction}
Newton's method uses second-order curvature information to achieve quadratic local convergence.
\subsection{Details}
Expanding $f$ around $x_k$ using a second-order Taylor series and minimizing the quadratic model yields the update
\begin{equation}
x_{k+1}=x_k-\left[\nabla^2 f(x_k)\right]^{-1}\nabla f(x_k).
\end{equation}
\subsection{Remarks}
Newton's method can be expensive in high dimensions due to the Hessian inverse, but forms the basis of quasi-Newton methods such as BFGS.

\section{Coordinate Descent}
\subsection{Introduction}
Coordinate descent optimizes one coordinate at a time while keeping others fixed.
\subsection{Update Rule}
For differentiable $f$, the $i$-th coordinate is updated by
\begin{equation}
x_{k+1}^{(i)} = x_k^{(i)} - \eta \frac{\partial f}{\partial x_i}(x_k),
\end{equation}
cycling through coordinates or selecting them randomly. This method is effective when gradient computations are cheaper coordinate-wise.

\section{Stochastic Optimization}
\subsection{Stochastic Gradient Descent}
\subsubsection{Derivation}
Let the empirical risk be $F(x)=\frac{1}{N}\sum_{i=1}^N f_i(x)$. Computing $\nabla F$ exactly requires a pass over the dataset. Instead, sample an index $i_k$ and form an unbiased estimator $g_k=\nabla f_{i_k}(x_k)$ so that $\mathbb{E}[g_k]=\nabla F(x_k)$. The update becomes
\begin{equation}
x_{k+1}=x_k-\eta_k g_k.
\end{equation}

\begin{algorithm}
\caption{Stochastic Gradient Descent}
\begin{algorithmic}[1]
\STATE Initialize $x_0$
\FOR{$k=0,1,2,\dots$}
    \STATE Sample $i_k$ and compute $g_k = \nabla f_{i_k}(x_k)$
    \STATE $x_{k+1}=x_k-\eta_k g_k$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Robbins--Monro Convergence \cite{robbins1951}]
Assume $F$ is convex with $L$-Lipschitz gradient and that $\mathbb{E}[\|g_k-\nabla F(x_k)\|_2^2]\le\sigma^2$. If step sizes satisfy $\sum_k \eta_k=\infty$ and $\sum_k \eta_k^2<\infty$, then $x_k\to x^*$ almost surely, where $x^*$ minimizes $F$.
\end{theorem}
\begin{proof}[Proof Sketch]
The updates form a stochastic approximation. Applying the Robbins--Siegmund lemma to the expected squared distance $\mathbb{E}[\|x_k-x^*\|^2]$ establishes almost sure convergence.
\end{proof}

\subsubsection{Mini-Batch SGD}
Rather than a single example, mini-batch SGD averages gradients over a small batch $B_k$, reducing variance:
\begin{equation}
g_k = \frac{1}{|B_k|}\sum_{i\in B_k} \nabla f_i(x_k).
\end{equation}

\subsubsection{Momentum \cite{polyak1964}}
Momentum accumulates a velocity term $v_{k+1}=\beta v_k + g_k$, leading to the update $x_{k+1}=x_k-\eta v_{k+1}$. This dampens oscillations and accelerates convergence.

\subsubsection{Nesterov's Accelerated Gradient \cite{nesterov1983}}
Nesterov proposed looking ahead by evaluating the gradient at $x_k-\beta v_k$, yielding improved convergence rates for smooth convex functions.

\subsubsection{AdaGrad \cite{duchi2011}}
AdaGrad adapts learning rates per coordinate using the accumulated squared gradients $G_k$, updating
\begin{equation}
x_{k+1}=x_k-\eta G_k^{-1/2} g_k.
\end{equation}
This method is well suited for sparse data.

\subsubsection{RMSProp \cite{tieleman2012}}
RMSProp maintains an exponential moving average of squared gradients $s_{k+1}=\rho s_k+(1-\rho)g_k^2$ and scales the update by $1/\sqrt{s_{k+1}+\epsilon}$, mitigating AdaGrad's aggressively decaying rates.

\subsubsection{Adam \cite{kingma2015}}
Adam combines momentum and RMSProp-style adaptive scaling.

\begin{algorithm}
\caption{Adam}
\begin{algorithmic}[1]
\STATE Initialize $x_0$, $m_0=0$, $v_0=0$
\FOR{$k=0,1,2,\dots$}
    \STATE Sample $i_k$ and compute $g_k=\nabla f_{i_k}(x_k)$
    \STATE $m_{k+1}=\beta_1 m_k + (1-\beta_1)g_k$
    \STATE $v_{k+1}=\beta_2 v_k + (1-\beta_2)g_k^2$
    \STATE $\hat{m}_{k+1}=m_{k+1}/(1-\beta_1^{k+1})$
    \STATE $\hat{v}_{k+1}=v_{k+1}/(1-\beta_2^{k+1})$
    \STATE $x_{k+1}=x_k-\eta\,\hat{m}_{k+1}/(\sqrt{\hat{v}_{k+1}}+\epsilon)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Convergence and Trade-offs}
Stochastic methods trade per-iteration accuracy for computational efficiency. Mini-batching reduces gradient variance, while momentum and adaptive methods accelerate training but may sacrifice theoretical guarantees. In practice, careful tuning of learning rates and batch sizes is essential.

\section{Min--Max Optimization}
\subsection{Introduction}
Min--max problems involve two agents with opposing objectives. Formally, one seeks
\begin{equation}
\min_{\theta} \max_{\phi} L(\theta,\phi).
\end{equation}

\subsection{Why Needed}
GANs cast generative modeling as an adversarial game between a generator $G$ and discriminator $D$. Training corresponds to finding a saddle point of the loss landscape.

\subsection{Details}
A pair $(\theta^*,\phi^*)$ is a \emph{saddle point} if
\begin{equation}
L(\theta^*,\phi)\le L(\theta^*,\phi^*)\le L(\theta,\phi^*)\quad\forall\theta,\phi.
\end{equation}
Gradient descent-ascent methods update $\theta$ via descent on $L$ and $\phi$ via ascent.

\begin{algorithm}
\caption{Gradient Descent--Ascent}
\begin{algorithmic}[1]
\STATE Initialize $\theta_0, \phi_0$
\FOR{$k=0,1,2,\dots$}
    \STATE $\theta_{k+1}=\theta_k-\eta_\theta\nabla_{\theta}L(\theta_k,\phi_k)$
    \STATE $\phi_{k+1}=\phi_k+\eta_\phi\nabla_{\phi}L(\theta_k,\phi_k)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Theorems and Contributions}
\begin{theorem}[von Neumann's Minimax Theorem \cite{vonneumann1928}]
For a finite zero-sum game with payoff matrix $A$, the value of the game satisfies
\begin{equation}
\min_{p}\max_{q} p^T A q = \max_{q}\min_{p} p^T A q,
\end{equation}
where $p$ and $q$ are mixed strategies. This establishes existence of Nash equilibria.
\end{theorem}
This game-theoretic foundation underlies the design of GAN objectives.

\section{Constrained Optimization}
Many learning problems involve constraints such as norm bounds or probability simplices.
\subsection{Lagrangian and KKT Conditions}
Consider $\min_x f(x)$ subject to $g_i(x)\le0$ and $h_j(x)=0$. The Lagrangian is
\begin{equation}
\mathcal{L}(x,\lambda,\nu)=f(x)+\sum_i \lambda_i g_i(x)+\sum_j \nu_j h_j(x).
\end{equation}
Under suitable regularity, Karush--Kuhn--Tucker (KKT) conditions characterize optimality: primal feasibility, dual feasibility $\lambda_i\ge0$, complementary slackness $\lambda_i g_i(x)=0$, and stationarity $\nabla_x \mathcal{L}=0$.
\subsection{Duality}
The dual function $d(\lambda,\nu)=\inf_x \mathcal{L}(x,\lambda,\nu)$ yields a lower bound on $f^*$. Strong duality holds for convex problems satisfying Slater's condition, enabling efficient solutions via dual ascent.

\section{Subgradient and Proximal Methods}
For nonsmooth convex $f$, a subgradient $g$ satisfies $f(y)\ge f(x)+\langle g,y-x\rangle$. Subgradient descent updates $x_{k+1}=x_k-\eta_k g_k$. Proximal methods handle composite objectives $f(x)=g(x)+h(x)$ with easy proximal operator $\operatorname{prox}_{\eta h}(v)=\arg\min_x h(x)+\tfrac1{2\eta}\|x-v\|^2$, leading to algorithms like ISTA and ADMM used in sparse coding and variational formulations of diffusion models.

\section{Convergence Rates}
For convex $f$ with $L$-Lipschitz gradients, gradient descent achieves $O(1/k)$ convergence, while Nesterov acceleration attains $O(1/k^2)$. Strong convexity yields linear rates $O((1-\mu/L)^k)$. These rates guide learning rate schedules in training deep networks.

\section{Variance Reduction Techniques}
Stochastic gradients suffer from high variance. Variance-reduced methods maintain control variates.
\subsection{SVRG \cite{johnson2013}}
Stochastic Variance Reduced Gradient periodically computes the full gradient $\tilde{\mu}$ and uses
\begin{equation}
g_k=\nabla f_{i_k}(x_k)-\nabla f_{i_k}(\tilde{x})+\tilde{\mu}
\end{equation}
to reduce variance while retaining unbiasedness.
\subsection{SAGA \cite{defazio2014}}
SAGA stores gradients for each data point to construct a control variate, achieving linear convergence for strongly convex objectives.

\section{Adaptive Methods: Theory}
Adaptive algorithms like Adam modify learning rates per coordinate. Under assumptions of bounded gradients and appropriate decay parameters, convergence to stationary points can be proved for convex problems, though general nonconvex guarantees remain open. Understanding these methods informs practical training of VAEs and GANs.

\chapter{Stochastic Processes and Diffusion}
Stochastic processes model the evolution of random variables over time. Diffusion-based generative models rely heavily on concepts from Brownian motion and stochastic differential equations.

\section{Overview of Stochastic Processes}
A stochastic process $\{X_t\}_{t\ge0}$ is a collection of random variables indexed by time. Processes may be discrete- or continuous-time and can have state spaces ranging from finite sets to $\mathbb{R}^d$. Key properties include stationarity and memory. Understanding these notions prepares us for Markov models used in diffusion and autoregressive techniques.

\section{Markov Chains}
Discrete-time Markov chains satisfy $\mathbb{P}(X_{t+1}=j\mid X_t=i,\ldots)=P_{ij}$ where $P$ is a transition matrix with rows summing to one. The chain is \emph{irreducible} if every state communicates and \emph{aperiodic} if returns occur at irregular times. The stationary distribution $\pi$ solves $\pi^T P=\pi^T$. Markov chains model sampling procedures in MCMC and are discrete analogues of diffusion processes.

\section{Brownian Motion}
\subsection{Definition and Properties}
\begin{definition}[Brownian Motion]
A stochastic process $\{B_t\}_{t\ge0}$ is Brownian motion if $B_0=0$, it has independent increments, $B_{t+s}-B_s\sim\mathcal{N}(0,t)$, and paths are almost surely continuous.
\end{definition}
Brownian motion, first observed by Robert Brown (1827) and mathematically formalized by Wiener (1923), is the canonical continuous-time stochastic process.

\subsection{Why Needed}
Diffusion models construct forward processes by adding Gaussian noise resembling Brownian motion. Understanding its properties allows connection to partial differential equations that describe density evolution.

\section{Itô Calculus}
\subsection{Stochastic Integrals}
For a stochastic process $X_t$ adapted to the filtration of $B_t$, the Itô integral is defined as the mean-square limit
\begin{equation}
\int_0^T X_t \, dB_t = \lim_{n\to\infty} \sum_{k=0}^{n-1} X_{t_k} (B_{t_{k+1}}-B_{t_k}).
\end{equation}

\subsection{Itô's Formula}
\begin{theorem}[Itô's Formula \cite{ito1944}]
If $X_t$ satisfies $dX_t=\mu(t,X_t)dt+\sigma(t,X_t)dB_t$ and $f\in C^{2}$, then
\begin{equation}
df(X_t) = \left(\frac{\partial f}{\partial t}+\mu\frac{\partial f}{\partial x}+\frac{1}{2}\sigma^2\frac{\partial^2 f}{\partial x^2}\right)dt + \sigma\frac{\partial f}{\partial x} dB_t.
\end{equation}
\end{theorem}
Itô calculus is indispensable for deriving diffusion and score-based models.

\section{Fokker--Planck Equation}
Let $X_t$ satisfy the stochastic differential equation (SDE)
\begin{equation}
dX_t = f(X_t,t) dt + g(X_t,t) dB_t.
\end{equation}
The probability density $p(x,t)$ of $X_t$ solves the Fokker--Planck equation
\begin{equation}
\frac{\partial p}{\partial t} = -\nabla_x\cdot (f p) + \frac{1}{2}\nabla_x^2:(gg^T p),
\end{equation}
which describes how densities evolve over time. Diffusion models design $f$ and $g$ to allow tractable reverse-time sampling.

\section{Ornstein--Uhlenbeck Process}
The Ornstein--Uhlenbeck (OU) process satisfies
\begin{equation}
dX_t = -\gamma X_t dt + \sigma dB_t,
\end{equation}
and has stationary distribution $\mathcal{N}(0, \sigma^2/(2\gamma))$. OU dynamics model velocity in Langevin equations and appear in continuous-time auto-regressive priors.

\section{Langevin Dynamics}
Langevin dynamics describe sampling from $p(x)\propto e^{-U(x)}$ via
\begin{equation}
dX_t = -\nabla U(X_t) dt + \sqrt{2} dB_t.
\end{equation}
Discretizing yields the unadjusted Langevin algorithm used for MCMC in EBMs. Adding momentum produces Hamiltonian Monte Carlo.

\section{Time Reversal of SDEs}
For an SDE $dX_t=f(X_t,t)dt+g(t)dB_t$ with density $p_t$, Anderson \cite{anderson1982} showed the time-reversed process satisfies
\begin{equation}
dX_t = \left[f(X_t,t)-g^2(t)\nabla_x \log p_t(X_t)\right]dt + g(t) d\bar{B}_t,
\end{equation}
where $\bar{B}_t$ is backward Brownian motion. This result underpins diffusion model sampling.

\section{Girsanov's Theorem}
Girsanov \cite{girsanov1960} provides a change-of-measure formula: if $X_t$ satisfies $dX_t = f dt + g dB_t$ and $u_t$ is adapted with $\int_0^T \|u_t\|^2 dt < \infty$, then under the new measure defined by
\begin{equation}
\frac{d\mathbb{Q}}{d\mathbb{P}} = \exp\left(\int_0^T u_t^T dB_t - \frac12\int_0^T \|u_t\|^2 dt\right),
\end{equation}
the process $\tilde{B}_t = B_t - \int_0^t u_s ds$ is Brownian. Girsanov's theorem allows incorporating drift changes and is used in score-based diffusion analyses.

\chapter{Generative Models}
We now study key generative architectures, emphasizing intuition, mathematics, and algorithms.

\section{Variational Autoencoders}
\subsection{Introduction and Motivation}
Variational Autoencoders (VAEs), introduced by Kingma and Welling (2013), combine deep learning with variational inference to learn latent-variable models. VAEs maximize a lower bound on the log-likelihood of data.

\subsection{Mathematical Foundations}
Given data $x\sim p_\text{data}(x)$, assume a latent variable $z$ with prior $p(z)$. The generative model is $p_\theta(x|z)$ parameterized by neural networks. Exact posterior inference $p_\theta(z|x)$ is intractable; VAEs introduce an approximate posterior $q_\phi(z|x)$.

\subsection{ELBO Derivation}
Starting from $\log p_\theta(x)=\log \int p_\theta(x,z)\, dz$ and introducing $q_\phi(z|x)$, we have
\begin{align}
\log p_\theta(x) &= \log \int q_\phi(z|x) \frac{p_\theta(x,z)}{q_\phi(z|x)} dz \\ 
&\ge \int q_\phi(z|x) \log \frac{p_\theta(x,z)}{q_\phi(z|x)} dz = \mathbb{E}_{q_\phi}[\log p_\theta(x|z)] - D_{\mathrm{KL}}(q_\phi(z|x)\|p(z)),
\end{align}
where Jensen's inequality yields the lower bound. The gap equals $D_{\mathrm{KL}}(q_\phi(z|x)\|p_\theta(z|x))$. Thus we obtain the Evidence Lower Bound (ELBO).

\begin{theorem}[Evidence Lower Bound]
For any $q_\phi(z|x)$,
\begin{align}
\log p_\theta(x) &= D_{\mathrm{KL}}(q_\phi(z|x)\|p_\theta(z|x)) + \mathbb{E}_{q_\phi} [\log p_\theta(x,z) - \log q_\phi(z|x)] \\
&\ge \mathbb{E}_{q_\phi} [\log p_\theta(x|z)] - D_{\mathrm{KL}}(q_\phi(z|x)\|p(z)) =: \mathcal{L}_{\text{ELBO}}(\theta,\phi).
\end{align}
\end{theorem}
\begin{proof}
Rearrange the definition of KL divergence $D_{\mathrm{KL}}(q\|p)\ge0$.
\end{proof}

To optimize the ELBO, one samples $z$ from $q_\phi(z|x)$ using the \emph{reparameterization trick}: for $q_\phi(z|x)=\mathcal{N}(\mu_\phi(x),\Sigma_\phi(x))$, write $z=\mu_\phi(x)+\Sigma_\phi^{1/2}(x)\epsilon$ with $\epsilon\sim\mathcal{N}(0,I)$.

\begin{algorithm}
\caption{VAE Training}
\begin{algorithmic}[1]
\FOR{each mini-batch $\{x_i\}$}
    \STATE Sample $\epsilon_i\sim\mathcal{N}(0,I)$
    \STATE Set $z_i=\mu_\phi(x_i)+\Sigma_\phi^{1/2}(x_i)\epsilon_i$
    \STATE Compute loss $\mathcal{L}_{\text{ELBO}}$
    \STATE Update $\theta,\phi$ via gradient ascent
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Strengths, Limitations, and Open Questions}
VAEs provide principled density estimation with efficient inference but often produce blurry samples due to the Gaussian likelihood assumption. Research directions include richer posteriors, hierarchical VAEs, and combining VAEs with flows or diffusion models.

\section{Generative Adversarial Networks}
\subsection{Introduction and Motivation}
Generative Adversarial Networks (GANs) were proposed by Goodfellow et al. (2014). A generator network $G$ maps latent noise $z\sim p(z)$ to data space, while a discriminator $D$ attempts to distinguish real samples from generated ones.

\subsection{Mathematical Foundations}
The classic GAN objective is
\begin{equation}
\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{\text{data}}} [\log D(x)] + \mathbb{E}_{z\sim p(z)} [\log(1-D(G(z)))].
\end{equation}

\begin{theorem}[Optimal Discriminator \cite{goodfellow2014}]
For fixed $G$, the optimal discriminator is $D^*(x)=\frac{p_{\text{data}}(x)}{p_{\text{data}}(x)+p_G(x)}$.
\end{theorem}
\begin{proof}
Differentiate $V(D,G)$ with respect to $D(x)$ and set to zero.
\end{proof}
Substituting $D^*$ yields $V(D^*,G)= -\log 4 + 2\,\text{JS}(p_{\text{data}}\|p_G)$, showing GANs minimize Jensen--Shannon divergence.

\subsection{Probability Metrics}
Beyond Jensen--Shannon divergence, other distances compare distributions. The \emph{Wasserstein-1} distance \cite{villani2003} is
\begin{equation}
W_1(p,q)=\inf_{\gamma\in\Pi(p,q)} \int \|x-y\| d\gamma(x,y),
\end{equation}
where $\Pi(p,q)$ are couplings with marginals $p$ and $q$. Wasserstein GANs replace the JS divergence with $W_1$, yielding a smoother objective linked to optimal transport. Choice of metric affects training stability and sample quality.

\subsection{Algorithm}
\begin{algorithm}
\caption{Stochastic GAN Training}
\begin{algorithmic}[1]
\FOR{each step}
    \STATE Sample $\{x_i\}\sim p_{\text{data}}$, $\{z_i\}\sim p(z)$
    \STATE Update discriminator by ascending $\nabla_D$ of objective
    \STATE Update generator by descending $\nabla_G$ of objective
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Strengths, Limitations, and Open Questions}
GANs generate sharp images but suffer from training instability and mode collapse. Research explores Wasserstein GANs, regularization, and alternative divergences. Understanding dynamics of adversarial optimization remains open.

\section{Diffusion Models}
\subsection{Introduction and Motivation}
Diffusion models, beginning with Sohl-Dickstein et al. (2015) and popularized by Ho et al. (2020), generate samples by reversing a gradual noising process described by an SDE or Markov chain.

\subsection{Mathematical Foundations}
Consider a forward process $q(x_t|x_{t-1})=\mathcal{N}(\sqrt{1-\beta_t}x_{t-1},\beta_t I)$ adding Gaussian noise. As $t\to T$, $x_T$ approaches an isotropic Gaussian. The reverse process is parameterized by a neural network that predicts the score $\nabla_x \log q(x_t)$.

Using results from score matching \cite{hyvarinen2005} and Fokker--Planck theory, the reverse SDE can be written as
\begin{equation}
dX_t = \left[f(X_t,t) - g^2(t)\nabla_x \log q_t(X_t)\right]dt + g(t)d\bar{B}_t,
\end{equation}
where $\bar{B}_t$ is Brownian motion run backward in time.

\subsection{Probability Flow ODEs and Score Matching}
The same marginal distributions arise from the deterministic probability flow ODE
\begin{equation}
\frac{dX_t}{dt}=f(X_t,t)-\tfrac12 g^2(t)\nabla_x \log q_t(X_t),
\end{equation}
which enables likelihood evaluation via the instantaneous change-of-variables formula. Score matching estimates $\nabla_x\log q_t$ without normalizing constants by minimizing $\mathbb{E}_{q_t}[\|s_\theta(x,t)-\nabla_x\log q_t(x)\|^2]$. These ideas connect diffusion models to normalizing flows and EBMs.

\subsection{Algorithm (DDPM)}
\begin{algorithm}
\caption{Denoising Diffusion Probabilistic Model}
\begin{algorithmic}[1]
\FOR{$t=T,\dots,1$}
    \STATE Predict noise $\epsilon_\theta(x_t,t)$
    \STATE Compute mean $\mu_t=\frac{1}{\sqrt{1-\beta_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta)$
    \STATE Sample $x_{t-1}\sim\mathcal{N}(\mu_t,\sigma_t^2 I)$
\ENDFOR
\RETURN $x_0$
\end{algorithmic}
\end{algorithm}

\subsection{Strengths, Limitations, and Open Questions}
Diffusion models achieve state-of-the-art sample quality and likelihoods but are computationally expensive due to long sampling chains. Active research investigates faster samplers, theoretical understanding of score estimation, and connections to ODE-based flows.

\section{Energy-Based Models}
\subsection{Introduction and Motivation}
Energy-Based Models (EBMs) assign an unnormalized energy $E_\theta(x)$ to each configuration $x$, defining $p_\theta(x)=Z^{-1}\exp(-E_\theta(x))$. Learning seeks low energy for data and high energy elsewhere. Early examples include the Boltzmann machine (Ackley et al., 1985).

\subsection{Mathematical Foundations}
The log-likelihood gradient is
\begin{equation}
\nabla_\theta \log p_\theta(x)= -\nabla_\theta E_\theta(x)+\mathbb{E}_{p_\theta}[\nabla_\theta E_\theta(X)].
\end{equation}
The partition function $Z=\int e^{-E_\theta(x)}dx$ is typically intractable, necessitating approximate methods like Markov Chain Monte Carlo (MCMC).

\subsection{MCMC and Partition Functions}
MCMC constructs a Markov chain with stationary distribution proportional to $e^{-E_\theta(x)}$. Algorithms such as Gibbs sampling and Langevin dynamics approximate expectations in the log-likelihood gradient. The intractable partition function cancels in ratio-based methods and its gradients can be estimated by importance sampling or annealed importance sampling. Efficient sampling is crucial for scaling EBMs to high dimensions.

\subsection{Contrastive Divergence}
Hinton's contrastive divergence (2002) approximates the log-likelihood gradient using short-run MCMC chains.
\begin{algorithm}
\caption{Contrastive Divergence-$k$}
\begin{algorithmic}[1]
\FOR{each data point $x$}
    \STATE Run $k$ steps of MCMC starting at $x$ to obtain $\tilde{x}$
    \STATE Update $\theta$ by $\nabla_\theta E_\theta(x)-\nabla_\theta E_\theta(\tilde{x})$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Strengths, Limitations, and Open Questions}
EBMs provide flexible energy shaping and can model complex dependencies but suffer from difficult normalization and sampling. Recent work explores score-based EBMs and connections to diffusion models.

\section{Autoregressive Models}
\subsection{Introduction and Motivation}
Autoregressive models factorize densities as $p(x)=\prod_{i=1}^n p(x_i\mid x_{<i})$. Examples include PixelCNN and transformer-based language models.
\subsection{Mathematical Details}
Training maximizes the log-likelihood
\begin{equation}
\log p(x)=\sum_{i=1}^n \log p(x_i\mid x_{<i}),
\end{equation}
using teacher forcing to expose ground-truth prefixes. The chain rule ensures exact likelihoods and efficient sampling but limits parallel generation.
\subsection{Connections}
Autoregressive factorization links to normalizing flows through triangular Jacobians and forms the backbone of diffusion model parameterizations such as transformer-based denoisers.

\section{Normalizing Flows}
\subsection{Introduction and Motivation}
Normalizing flows \cite{rezende2015} transform a simple base distribution $z\sim p_z$ into a complex $x$ via an invertible map $x=T_\theta(z)$.
\subsection{Mathematical Foundations}
The change-of-variables formula gives
\begin{equation}
\log p_\theta(x)=\log p_z(T_\theta^{-1}(x)) + \log\left|\det \frac{\partial T_\theta^{-1}}{\partial x}\right|.
\end{equation}
Flows enable exact likelihoods and efficient sampling when the Jacobian determinant is tractable, as in coupling layers (RealNVP) or autoregressive transforms (MADE). They connect to diffusion through continuous normalizing flows governed by probability flow ODEs.

\chapter{Further Concepts}
\begin{itemize}
    \item \textbf{Normalizing Flows} \cite{rezende2015}: invertible transformations with tractable Jacobian determinants enabling exact likelihood evaluation.
    \item \textbf{Score Matching} \cite{hyvarinen2005}: estimators based on matching gradients of log densities, foundational for diffusion models and EBMs.
    \item \textbf{Stein Variational Gradient Descent} \cite{liu2016}: particle-based variational inference using Stein's identity.
    \item \textbf{Connections to Reinforcement Learning and Physics}: generative models relate to control problems, statistical mechanics, and thermodynamics, offering a rich interdisciplinary research area.
\end{itemize}

\bibliographystyle{plain}
\begin{thebibliography}{99}
\bibitem{cauchy1821} A.-L. Cauchy. \emph{Cours d'Analyse}, 1821.
\bibitem{schwarz1888} H. A. Schwarz. \emph{Gesammelte Mathematische Abhandlungen}, 1888.
\bibitem{hilbert1904} D. Hilbert. \emph{Grundzüge einer Allgemeinen Theorie der Linearen Integralgleichungen}, 1904.
\bibitem{bernoulli1713} J. Bernoulli. \emph{Ars Conjectandi}, 1713.
\bibitem{laplace1812} P.-S. Laplace. \emph{Théorie Analytique des Probabilités}, 1812.
\bibitem{lyapunov1901} A. Lyapunov. "Uber den zentralen Grenzwertsatz der Wahrscheinlichkeitsrechnung", 1901.
\bibitem{kullback1951} S. Kullback and R. A. Leibler. "On information and sufficiency", \emph{Ann. Math. Statist.}, 1951.
\bibitem{gibbs1873} J. W. Gibbs. \emph{A Method of Geometrical Representation of Thermodynamic Properties}, 1873.
\bibitem{shannon1948} C. E. Shannon. "A mathematical theory of communication", \emph{Bell System Technical Journal}, 1948.
\bibitem{jensen1906} J. L. W. V. Jensen. "Sur les fonctions convexes", \emph{Acta Mathematica}, 1906.
\bibitem{cauchy1847} A.-L. Cauchy. "Méthode générale pour la résolution des systèmes d'équations simultanées", 1847.
\bibitem{rockafellar1970} R. T. Rockafellar. \emph{Convex Analysis}, 1970.
\bibitem{vonneumann1928} J. von Neumann. "Zur Theorie der Gesellschaftsspiele", 1928.
\bibitem{ito1944} K. Itô. "Stochastic integral", \emph{Proc. Imp. Acad. Tokyo}, 1944.
\bibitem{hyvarinen2005} A. Hyvärinen. "Estimation of non-normalized statistical models by score matching", \emph{JMLR}, 2005.
\bibitem{kingma2013} D. P. Kingma and M. Welling. "Auto-encoding variational Bayes", 2013.
\bibitem{goodfellow2014} I. Goodfellow et al. "Generative adversarial nets", 2014.
\bibitem{sohldickstein2015} J. Sohl-Dickstein et al. "Deep unsupervised learning using nonequilibrium thermodynamics", 2015.
\bibitem{ho2020} J. Ho et al. "Denoising diffusion probabilistic models", 2020.
\bibitem{rezende2015} D. J. Rezende and S. Mohamed. "Variational inference with normalizing flows", 2015.
\bibitem{liu2016} Q. Liu and D. Wang. "Stein variational gradient descent", 2016.
\bibitem{robbins1951} H. Robbins and S. Monro. "A stochastic approximation method", \emph{Ann. Math. Statist.}, 1951.
\bibitem{polyak1964} B. T. Polyak. "Some methods of speeding up the convergence of iteration methods", \emph{USSR Comput. Math. Math. Phys.}, 1964.
\bibitem{nesterov1983} Y. Nesterov. "A method for unconstrained convex minimization problem with the rate of convergence $O(1/k^2)$", \emph{Dokl. Akad. Nauk SSSR}, 1983.
\bibitem{duchi2011} J. Duchi, E. Hazan, and Y. Singer. "Adaptive subgradient methods for online learning and stochastic optimization", \emph{JMLR}, 2011.
\bibitem{tieleman2012} T. Tieleman and G. Hinton. "Lecture 6.5 -- RMSProp: Divide the gradient by a running average of its recent magnitude", COURSERA: Neural Networks for Machine Learning, 2012.
\bibitem{kingma2015} D. P. Kingma and J. Ba. "Adam: A method for stochastic optimization", 2015.
\bibitem{villani2003} C. Villani. \emph{Topics in Optimal Transportation}, 2003.
\bibitem{johnson2013} R. Johnson and T. Zhang. "Accelerating stochastic gradient descent using predictive variance reduction", \emph{NIPS}, 2013.
\bibitem{defazio2014} A. Defazio, F. Bach, and S. Lacoste-Julien. "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", \emph{NIPS}, 2014.
\bibitem{anderson1982} B. D. O. Anderson. "Reverse-time diffusion equation models", \emph{Stochastics}, 1982.
\bibitem{girsanov1960} I. V. Girsanov. "On transforming a certain class of stochastic processes by absolutely continuous substitution of measures", \emph{Theory of Probability and Its Applications}, 1960.
\end{thebibliography}

\end{document}
